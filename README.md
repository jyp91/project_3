# Web Scraping Reddit.com Project 3

Reddit is a very popular American social news aggregation, web content rating, and discussion website. In this project, all data gathered from Reddit.com by building a web scraper with Python's library BeautifulSoup. With gathered data, it uses RandomForest Classifier and Logistic Regression with Grid-Search to predict the user engagement (high comment vs low comment) and use Confusion Matrix to check the accuracy of the models. 

**Executive Summary | What Drives High Number of Comments on Reddit?**

By Jin Park

**Abstract:** This analysis sought to understand what type of features in Reddit post that will get the most engagement from the Reddit users. First, the analyst gathered all necessary data by web scraping the website Reddit.com. Additionally, the analyst looked at which features could bring the most engagement from the Reddit users by extracting four interesting words in the title, subreddits, time when the post was posted, and title length on average. The models showed that the best feature to use to determine the most engagement was subreddits. On the other hand, using other features only performed where the baseline result was 75%. Overall, models are more accurate on predicting the low comments than the high comments.

**Introduction:** The analyst pulled all the necessary data from Reddit.com, an American social news aggregation, web content rating, and discussion online website. To pull the data, a web scraper was built using Python and BeautifulSoup library. The web scraper pulled over 40 pages that contain every 1000 results on subreddits, times, number of comments, and titles. Please note that size of this dataset is very small compared to other industrial datasets due to lack of computational power to web scrape more data. With the pulled dataset, an analysis was performed by using many different models such as Random Forest, Logistic Regression, Confusion Matrix, Grid-Search, Cross-Validation, and Count Vectorizer to predict what features determine the high comments from the Reddit users.

**Methods:** All the features in the datasets are converted into binary classification problem into 1 or 0. Also, the target or the high number of comments was set by using the 75th percentile for the number of comments as a breaking point. Any comments that are higher than 401 comments were converted into 1 and anything below as 0. In addition, subreddits were converted into binary classification by converting into dummy variables. Also, four words were randomly selected by the analyst that analyst believes that it can impact on predicting the high number of comments. Moreover, time feature was also converted into a binary classification by converting all the post that was posted after 12:00 pm into 1 and others to 0. Lastly, the average length of the title feature was also converted into a binary by calculating the average length for the title, 70 characters. Any titles that were longer than 70 was converted into 1 and else 0. The analysts used Random Forest Model, Logistic Regression, Confusion Matrix and Count Vectorizer to model then used Grid Search and Cross Validation to reduce variance to optimize the models result. The models analyzed how the selected 4 words in title, subreddits, time posted in the noon or morning, and length of the titles on average could affect the high number of comments.

**Results:** The Random Forest model that uses subreddit as a feature with 5 folds Cross-Validation performed the best out of all models for predicting the high number of comments. However, using Count Vectorizer and Random Forest model, using the title as a feature performed the best on predicting the high number of comments without performing the Cross-Validation. This may indicate that the model could be overfitted since it did not use any model optimization method to reduce variance. The features using 4 selected words, the time when the post was posted in the noon or morning, and title length on average performed only where the baseline was which is 75%. This indicates that these features do not generate any values for predicting the high number of comments since it cannot outperform the baseline accuracy. This could be due to four randomly selected words did not show up often in titles to bring any impact on the prediction. Also, time and title length do not have any relationship to predicting the high number of comments. Lastly, based on the confusion matrix results, models are better at predicting the low number of comments then the high number of comments. For example, overall models specificity score shows close to 100%.

**Recommendations:** Since the dataset was too small, to avoid this problem, run web scraper each day or every other day for few weeks or months to gather enough data. When gathering data, save each result into a csv or txt file and aggregate all gathered csv files into one file to increase the size of the data. By increasing the dataset, this will automatically increase accuracy in predicting the target value. Based on the models result, features that did not perform well on predicting the high number of comments can be used to predict low number of comments. Use this features to determine what drives the low number of comments to understand what could effect the high number of comments.
